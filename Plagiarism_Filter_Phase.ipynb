{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eejo29ezLNXx"
      },
      "source": [
        "# For Cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2E9P5qsuLQgW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "7ab6f8d1-0036-4027-d99d-e0c194d3d0e1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-1-4052150592.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1-4052150592.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    scp -J yaron.otmazgin@bava.cs.huji.ac.il \\\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "scp -J yaron.otmazgin@bava.cs.huji.ac.il \\\n",
        "    yaron.otmazgin@moriah-gw.cs.huji.ac.il:/sci/labs/orzuk/orzuk/teaching/big_data_project_52017/2024_25/arxiv_data/arxiv-metadata-oai-snapshot.json.zip \\\n",
        "    ~/Downloads/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjWqXhv2Z0HB"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "3tk9A9qGSVGi",
        "outputId": "535b2eb5-d7b9-4b09-ea43-bd0e6a41110d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3-1850705363.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "# relevant libraries for reading the JSON and converting to pandas\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# relevant libraries for the filter model\n",
        "\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHSiZrL3Z2m4"
      },
      "source": [
        "# Reading the JSON and converting to DF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IntgtE6eXgVZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Read the JSON lines file\n",
        "articles = []\n",
        "with open('/content/drive/My Drive/arxiv_sample.json', 'r') as f:\n",
        "    for line in f:\n",
        "        articles.append(json.loads(line))\n",
        "\n",
        "# convert from JSON to dataframe\n",
        "df = pd.DataFrame(articles)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IlYPpNYTfG1"
      },
      "outputs": [],
      "source": [
        "# view an example using the JSON format\n",
        "print(json.dumps(articles[0], indent=2))  # Pretty print first article"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4u1dmggUT2aF"
      },
      "outputs": [],
      "source": [
        "# view the head of the dataframe format\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w5Cu45XZxzv"
      },
      "source": [
        "# Filter Phase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNcVBlv8e4Yg"
      },
      "source": [
        "## Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "D3lNhJQ7WPGy",
        "outputId": "4cb15e71-91d2-4b40-a8fa-a24c7e918861"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-1183858834.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m# Run the pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mcluster_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msemantic_prefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m# Print some basic statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# 1. Document Embedding\n",
        "def create_embeddings(abstracts):\n",
        "    # Load pre-trained SBERT model\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    # Generate embeddings for abstracts\n",
        "    embeddings = model.encode(abstracts)\n",
        "    return embeddings\n",
        "\n",
        "# 2. Dimension Reduction\n",
        "def reduce_dimensions(embeddings, n_components=5):\n",
        "    # Apply UMAP for dimensionality reduction\n",
        "    umap_model = UMAP(n_components=n_components,\n",
        "                      n_neighbors=15,\n",
        "                      min_dist=0.1,\n",
        "                      random_state=42)\n",
        "    reduced_embeddings = umap_model.fit_transform(embeddings)\n",
        "    return reduced_embeddings\n",
        "\n",
        "# 3. Document Clustering\n",
        "def cluster_documents(reduced_embeddings):\n",
        "    # Apply HDBSCAN for clustering\n",
        "    clusterer = HDBSCAN(min_cluster_size=10,\n",
        "                        min_samples=2,\n",
        "                        metric='euclidean')\n",
        "    clusters = clusterer.fit_predict(reduced_embeddings)\n",
        "    return clusters\n",
        "\n",
        "# 4. Simple function to get clusters and their documents\n",
        "def get_cluster_documents(df, clusters):\n",
        "    df['cluster'] = clusters\n",
        "    # Return a dictionary of cluster IDs mapped to document indices\n",
        "    cluster_docs = {}\n",
        "    unique_clusters = np.unique(clusters)\n",
        "    for cluster_id in unique_clusters:\n",
        "        if cluster_id != -1:  # Skip noise points (HDBSCAN assigns -1 to noise)\n",
        "            cluster_docs[cluster_id] = df[df['cluster'] == cluster_id].index.tolist()\n",
        "    return cluster_docs\n",
        "\n",
        "# Main function to run the pipeline\n",
        "def semantic_prefilter(df):\n",
        "    abstracts = df['abstract'].tolist()\n",
        "\n",
        "    # Step 1: Create embeddings\n",
        "    embeddings = create_embeddings(abstracts)\n",
        "\n",
        "    # Step 2: Reduce dimensions\n",
        "    reduced_embeddings = reduce_dimensions(embeddings)\n",
        "\n",
        "    # Step 3: Cluster documents\n",
        "    clusters = cluster_documents(reduced_embeddings)\n",
        "\n",
        "    # Step 4: Get clusters and their documents\n",
        "    cluster_docs = get_cluster_documents(df, clusters)\n",
        "\n",
        "    return cluster_docs, embeddings, clusters\n",
        "\n",
        "# Run the pipeline\n",
        "cluster_docs, embeddings, clusters = semantic_prefilter(df)\n",
        "\n",
        "# Print some basic statistics\n",
        "print(f\"Number of clusters found: {len(cluster_docs)}\")\n",
        "print(f\"Distribution of documents across clusters: {[len(docs) for docs in cluster_docs.values()]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8guZDJm9e9B5"
      },
      "source": [
        "## Visualizing clusters and common words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxwtC7dFZVkv"
      },
      "outputs": [],
      "source": [
        "def visualize_clusters(embeddings, clusters):\n",
        "    # First reduce to 2D for visualization regardless of previous reduction\n",
        "    umap_2d = UMAP(n_components=2, n_neighbors=15, min_dist=0.1, random_state=42)\n",
        "    vis_embeddings = umap_2d.fit_transform(embeddings)\n",
        "\n",
        "    # Create a DataFrame for easier plotting\n",
        "    vis_df = pd.DataFrame({\n",
        "        'x': vis_embeddings[:, 0],\n",
        "        'y': vis_embeddings[:, 1],\n",
        "        'cluster': clusters\n",
        "    })\n",
        "\n",
        "    # Set up plot size\n",
        "    plt.figure(figsize=(20, 16))\n",
        "\n",
        "    # Create a color palette that handles noise points (-1) separately\n",
        "    unique_clusters = np.unique(clusters)\n",
        "    num_clusters = len([c for c in unique_clusters if c != -1])\n",
        "    palette = sns.color_palette(\"hsv\", num_clusters)\n",
        "    colors = {i: palette[i] for i in range(num_clusters)}\n",
        "    colors[-1] = (0.5, 0.5, 0.5)  # Gray for noise points\n",
        "\n",
        "    # Plot each cluster\n",
        "    for cluster_id in unique_clusters:\n",
        "        cluster_data = vis_df[vis_df['cluster'] == cluster_id]\n",
        "        plt.scatter(\n",
        "            cluster_data['x'],\n",
        "            cluster_data['y'],\n",
        "            c=[colors[cluster_id]] * len(cluster_data),\n",
        "            label=f'Cluster {cluster_id}' if cluster_id != -1 else 'Noise',\n",
        "            alpha=0.7 if cluster_id != -1 else 0.3,\n",
        "            s=50\n",
        "        )\n",
        "\n",
        "    # Add labels and title\n",
        "    plt.title('Document Clusters Visualization', fontsize=16)\n",
        "    plt.xlabel('UMAP Dimension 1', fontsize=12)\n",
        "    plt.ylabel('UMAP Dimension 2', fontsize=12)\n",
        "\n",
        "    # Place the legend at the bottom of the plot\n",
        "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1),\n",
        "               ncol=min(13, len(unique_clusters)),  # Adjust number of columns based on cluster count\n",
        "               frameon=True, fancybox=True, shadow=True)\n",
        "\n",
        "    # Adjust layout to make room for the legend at the bottom\n",
        "    plt.tight_layout(rect=[0, 0.1, 1, 0.95])  # [left, bottom, right, top]\n",
        "\n",
        "    return plt\n",
        "\n",
        "def visualize_cluster_sample(df, embeddings, clusters, n_samples=3):\n",
        "    \"\"\"Visualize clusters and display sample titles from each cluster\"\"\"\n",
        "    plot = visualize_clusters(embeddings, clusters)\n",
        "\n",
        "    # Display sample documents from each cluster\n",
        "    print(\"Sample documents from each cluster:\")\n",
        "    unique_clusters = np.unique(clusters)\n",
        "    for cluster_id in unique_clusters:\n",
        "        if cluster_id == -1:\n",
        "            continue  # Skip noise points\n",
        "\n",
        "        cluster_docs = df[df['cluster'] == cluster_id]\n",
        "        sample_docs = cluster_docs.sample(min(n_samples, len(cluster_docs)))\n",
        "\n",
        "        print(f\"\\nCluster {cluster_id} ({len(cluster_docs)} documents):\")\n",
        "        for idx, row in sample_docs.iterrows():\n",
        "            print(f\"  - {row['title']}\")\n",
        "\n",
        "    plot.show()\n",
        "\n",
        "# Call the enhanced visualization function\n",
        "visualize_cluster_sample(df, embeddings, clusters)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7cJHqoWcPah"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "def get_top_words_per_cluster(df, clusters, n_words=3):\n",
        "    \"\"\"Get the most common words for each cluster\"\"\"\n",
        "    # Initialize stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Add some domain-specific stopwords that might be common in academic papers\n",
        "    domain_stops = {'using', 'paper', 'approach', 'method', 'model', 'propose', 'based', 'results', 'data', 'proposed'}\n",
        "    stop_words.update(domain_stops)\n",
        "\n",
        "    cluster_top_words = {}\n",
        "    unique_clusters = np.unique(clusters)\n",
        "\n",
        "    for cluster_id in unique_clusters:\n",
        "        if cluster_id == -1:  # Skip noise points\n",
        "            continue\n",
        "\n",
        "        # Get all abstracts for this cluster\n",
        "        cluster_docs = df[clusters == cluster_id]\n",
        "\n",
        "        # Combine all text\n",
        "        all_text = ' '.join(cluster_docs['abstract'].fillna(''))\n",
        "\n",
        "        # Basic preprocessing\n",
        "        all_text = all_text.lower()\n",
        "        # Remove special characters and numbers\n",
        "        all_text = re.sub(r'[^a-zA-Z\\s]', '', all_text)\n",
        "\n",
        "        # Tokenize\n",
        "        words = all_text.split()\n",
        "\n",
        "        # Remove stopwords\n",
        "        filtered_words = [word for word in words if word not in stop_words and len(word) > 3]\n",
        "\n",
        "        # Get most common words\n",
        "        word_counts = Counter(filtered_words)\n",
        "        top_words = [word for word, count in word_counts.most_common(n_words)]\n",
        "\n",
        "        cluster_top_words[cluster_id] = top_words\n",
        "\n",
        "    return cluster_top_words\n",
        "\n",
        "def visualize_cluster_words(df, embeddings, clusters, n_words=3):\n",
        "    # First reduce to 2D for visualization\n",
        "    umap_2d = UMAP(n_components=2, n_neighbors=15, min_dist=0.1, random_state=42)\n",
        "    vis_embeddings = umap_2d.fit_transform(embeddings)\n",
        "\n",
        "    # Get top words for each cluster\n",
        "    top_words = get_top_words_per_cluster(df, clusters, n_words)\n",
        "\n",
        "    # Calculate cluster centers\n",
        "    cluster_centers = {}\n",
        "    unique_clusters = np.unique(clusters)\n",
        "\n",
        "    for cluster_id in unique_clusters:\n",
        "        if cluster_id == -1:  # Skip noise points\n",
        "            continue\n",
        "\n",
        "        # Get points in this cluster\n",
        "        cluster_points = vis_embeddings[clusters == cluster_id]\n",
        "        # Calculate center\n",
        "        center_x = np.mean(cluster_points[:, 0])\n",
        "        center_y = np.mean(cluster_points[:, 1])\n",
        "\n",
        "        cluster_centers[cluster_id] = (center_x, center_y)\n",
        "\n",
        "    # Set up plot size - make it larger\n",
        "    plt.figure(figsize=(20, 16))\n",
        "\n",
        "    # Create a color palette\n",
        "    num_clusters = len([c for c in unique_clusters if c != -1])\n",
        "    palette = sns.color_palette(\"hsv\", num_clusters)\n",
        "    colors = {i: palette[i] for i in range(num_clusters)}\n",
        "\n",
        "    # First plot a faint scatter of all points to show distribution\n",
        "    for cluster_id in unique_clusters:\n",
        "        if cluster_id == -1:  # Skip noise\n",
        "            continue\n",
        "\n",
        "        cluster_points = vis_embeddings[clusters == cluster_id]\n",
        "        plt.scatter(\n",
        "            cluster_points[:, 0],\n",
        "            cluster_points[:, 1],\n",
        "            c=[colors[cluster_id]],\n",
        "            alpha=0.1,  # Very faint\n",
        "            s=10\n",
        "        )\n",
        "\n",
        "    # Now plot the words in circles\n",
        "    for cluster_id, center in cluster_centers.items():\n",
        "        if cluster_id not in top_words:\n",
        "            continue\n",
        "\n",
        "        cx, cy = center\n",
        "\n",
        "        # Draw a circle around all three words\n",
        "        circle = plt.Circle((cx, cy), 0.35, fill=True, alpha=0.2, color=colors[cluster_id],\n",
        "                            edgecolor=colors[cluster_id], linewidth=2)\n",
        "        plt.gca().add_patch(circle)\n",
        "\n",
        "        # Join the top words and display them in the center\n",
        "        word_text = '\\n'.join(top_words[cluster_id])\n",
        "\n",
        "        # Add the words\n",
        "        plt.text(cx, cy, word_text,\n",
        "                 fontsize=12,\n",
        "                 ha='center',\n",
        "                 va='center',\n",
        "                 color=colors[cluster_id],\n",
        "                 weight='bold',\n",
        "                 bbox=dict(facecolor='white', alpha=0.7, boxstyle='round,pad=0.3'))\n",
        "\n",
        "    # Add labels and title\n",
        "    plt.title('Cluster Topic Words Visualization', fontsize=20)\n",
        "    plt.xlabel('UMAP Dimension 1', fontsize=14)\n",
        "    plt.ylabel('UMAP Dimension 2', fontsize=14)\n",
        "\n",
        "    # Add a legend for cluster colors\n",
        "    legend_elements = []\n",
        "    for cluster_id in sorted(cluster_centers.keys()):\n",
        "        legend_elements.append(plt.Line2D([0], [0], marker='o', color='w',\n",
        "                               markerfacecolor=colors[cluster_id], markersize=10,\n",
        "                               label=f'Cluster {cluster_id}'))\n",
        "\n",
        "    plt.legend(handles=legend_elements, loc='upper center',\n",
        "               bbox_to_anchor=(0.5, -0.05), ncol=min(13, len(cluster_centers)))\n",
        "\n",
        "    # Remove axis ticks for cleaner look\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "    # Tight layout\n",
        "    plt.tight_layout()\n",
        "\n",
        "    return plt\n",
        "\n",
        "# Call the visualization function\n",
        "plot = visualize_cluster_words(df, embeddings, clusters)\n",
        "plot.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfSAYqaeSfAa"
      },
      "outputs": [],
      "source": [
        "clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aouTAfz8C8Ae"
      },
      "outputs": [],
      "source": [
        "# prompt: how do i know the number of samples in the json\n",
        "\n",
        "print(f\"Number of samples in the JSON: {len(articles)}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}